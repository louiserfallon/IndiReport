---
title: "Business Use Cases of Panel Scanner Data in the Retail Industry"
author: 'Louise Fallon - CID: 01262763'
output:
  pdf_document: default
  word_document: default
subtitle: 'Word Count: 5338'
header-includes: \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2) #for ggplot and plotting functions
library(mlogit) #for mlogit functions
library(data.table) # for data.table functions, aggregation etc.
library(gsubfn) #for gsubfn multiple-string-replace function 
library(knitr) #for kable markdown tables
source("mlogittotable.R")
```

\
\
\


##Abstract

The introduction of panel scanner data led to new analytical methods in the retail industry. This report describes, for the case where an analyst has access to panel scanner data for a manufacturer or retailer, what analysis can be done, how simple is this to do, and what business value it could provide. This is supported by an example using a sample of panel scanner data for carbonated beverages to illustrate possible outputs, insights, and corresponding business actions. Choice models are explored in detail, including techniques for calculating product share elasticity and promotional effectiveness. Extensions to the model and alternative modelling approaches are explored, and inclusion of social media data is discussed. 

\newpage

#Introduction

Panel scanner data is a category of retail data created by participating households repeatedly scanning products bought across a variety of stores over a period of time. Commonly referred to as UPC panel data, from the Universal Product Code that is scanned in the process, this kind of data became widely available to the field of marketing science throughout the 1980s (Guadagni & Little, 2008), and has been used for a variety of business use cases (Bucklin & Gupta, 1999). The introduction of UPC Data meant that households' individual purchasing decisions over time could be explored, rather than the aggregate store level sales data which had been predominant.

![Prevalence of "UPC scanner" in Google ngram corpus since 1970 (Google ngrams, 2008)](upcscannerngram.png)

A key use of UPC data is to build models of consumer choice. These models are not usually made to be predictive, but rather interpretative, to allow businesses to understand consumer behaviour. Choice models can estimate consumer reactions to different elements of the marketing mix. The outcome of the models often include a set of price and cross price elasticities, which can be used to visualise competitive structure. Consumer choice models can also be used to simulate consumer reactions to price, promotion, display and product changes, so retail professionals can test these changes before making the often costly decisions to bring the changes to market. 

This report uses an example of a carbonated soft drink data set to explain the steps that need to be taken in order to build a model of consumer choice, then explores the alternative extensions that could be made to the model. At each step, there is a discussion of the decisions that an analyst, building this type of model for a retailer or manufacturer, would need to make to ensure that the analysis provides business value. 

##Data Description and Preprocessing

```{r loaddata}
fulldata <- read.csv("../Data/CarbonatesBottlesandCans (confidential).csv", fileEncoding = "latin1")
```

```{r exploratoryanalysis, eval=FALSE}
#households
tmp <- as.data.frame(table(fulldata$HOUSE))
plot(tmp$Freq)
boxplot(tmp$Freq)
mean(tmp$Freq)
median(tmp$Freq) 
nrow(tmp) #29214 households
nrow(tmp[tmp$Freq>=5,])/nrow(tmp) #81% of households have more than 5 observations
nrow(tmp[tmp$Freq>=50,])/nrow(tmp) #25% of households have more than 50 observations

#stores
tmp <- as.data.frame(table(fulldata$shop_desc))
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
ggplot(tmp, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
nrow(tmp[tmp$Freq>=500,])/nrow(tmp) #95% of stores have more than 500 observations

#categories
tmp <- as.data.frame(table(fulldata$cat_name))
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
ggplot(tmp, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("all categories")

#subcategories
tmp <- as.data.frame(table(fulldata$sub_cat_name))
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
ggplot(tmp, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("top subcategories")

#brands
tmp <- as.data.frame(table(fulldata$brand_name))
nrow(tmp) #170 brands
tmp <- tmp[tmp$Freq > 1.5*mean(tmp$Freq),]
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
ggplot(tmp, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

#range
tmp <- as.data.frame(table(fulldata$total_range_name))
nrow(tmp) #215 ranges
tmp <- tmp[tmp$Freq > 1.5*mean(tmp$Freq),]
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
ggplot(tmp, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

#epromdesc
tmp <- as.data.frame(table(fulldata$epromdesc))
nrow(tmp) #362 promotions
tmp <- tmp[tmp$Freq > 1.5*mean(tmp$Freq),]
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
ggplot(tmp, aes(x=Var1, y=Freq)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
nrow(fulldata[!fulldata$epromdesc == "No Promotion",])/nrow(fulldata) #59% of observations have some promotion 

#looks like brand aggregates to store standard and value, whereas range has the actual stores (e.g. asda)
fulldata$date <- paste(fulldata$relweek,fulldata$DAY, sep="-")
length(unique(fulldata$date)) #364 days

##brand hierarchy (ensure bottom 3 include top 1)
fulldata$temp = paste(fulldata$total_range_name, fulldata$cat_name, fulldata$sub_cat_name)
brand <- aggregate(brand_name ~ temp, data=fulldata, FUN=function(x) length(unique(x)))
head(brand)

fulldata$volperpack = fulldata$Volume/fulldata$PACKS

```

```{r}
fulldata$date <- paste(fulldata$relweek,fulldata$DAY, sep="-")
fulldata$volperpack = fulldata$Volume/fulldata$PACKS

#take data only from the top retailer
data_ind_summary_pre <- fulldata[fulldata$shop_desc %in% c("1TESCO") &
                                fulldata$cat_name %in% c("Canned Colas","Canned Lemonade","Canned Other Flavours"),]


#prepare data to be aggregated
#by date, shop, household, range name, cat name and sub cat name
data_ind_summary_pre$product <- paste(data_ind_summary_pre$total_range_name, data_ind_summary_pre$sub_cat_name)
data_ind_summary_pre$key <- paste(data_ind_summary_pre$date, data_ind_summary_pre$shop_desc, data_ind_summary_pre$HOUSE, data_ind_summary_pre$product)
data_ind_summary_dt <- data.table(data_ind_summary_pre)
setkey(data_ind_summary_dt, key)

#aggregate and get:
#price per unit: count rows and sum net spend
#brand name tail, sub cat name tail, binary tail

df_ind_summary <- as.data.frame(data_ind_summary_dt[,
                        j=list(tail(date,1),
                               tail(HOUSE,1),
                               tail(shop_desc,1),
                               tail(product,1),
                               tail(sub_cat_name,1),
                               tail(cat_name,1),
                               tail(brand_name,1),
                               tail(epromdesc,1),
                               sum(Volume),
                               sum(NETSPEND,1)),
                        by=key])

colnames(df_ind_summary) <- c("key","date","HOUSE","shop_desc","product",
                             "sub_cat_name","cat_name", "brand_name", "epromdesc",
                             "Volume", "NETSPEND")
df_ind_summary$priceper330ml <- df_ind_summary$NETSPEND /  df_ind_summary$Volume *.33
```

```{r}
df_ind_summary$product[df_ind_summary$brand_name == "PL_Standard" & 
                         df_ind_summary$sub_cat_name == "Diet"] <- "Own Brand Diet"
df_ind_summary$product[df_ind_summary$brand_name == "PL_Standard" &
                         df_ind_summary$sub_cat_name == "Regular"] <- "Own Brand Regular"

tmp <- as.data.frame(table(df_ind_summary$product))
tmp$Var1 <- factor(tmp$Var1, levels = tmp$Var1[order(tmp$Freq, decreasing = TRUE)])
tmp <- tmp[order(tmp$Freq, decreasing = TRUE),]

#take top 22 products
tmpprods <- head(tmp$Var1,19)

df_ind_summary$product[!df_ind_summary$product %in% tmpprods] <- "Other"
```

The data used as an example of these techniques is made up of around 29,000 UK households' purchases of canned and bottled Carbonated Soft Drinks (CSDs) over a year, from December 2014 to December 2015 (hereafter referred to as the CSD data). The available variables are summarised in Table 1.

```{r}
kable(data.frame(variable=c("Date","Household", "Shop","Product", "Promotion","Volume","Price"),
                 description=c("Relative year, week and day of the shopping trip",
                               "Identifier of the household making the shopping trip",
                               "Name of the retailer",
                               "Category, Sub Category, Range and Brand Name",
                               "Description of promotion for that product at the time",
                               "Size (in litres) of the product",
                               "Amount spent on the product")), caption="CSD Data Variable Description")
```

The analysis uses only data from the top retailer, as more reliable data is available on product availability for this store, and to reduce computational run-time. Varying sizes of beverages are included, for example, 330ml cans and 2 litre bottles. The analysis is made meaningful by focusing only on the category of canned drinks, to focus on true competitors. Own brand products are treated as one product, and low volume products are grouped together to narrow the data to 20 products. The price range for these products can be seen in Figure 2. Energy drinks such as Red Bull have a higher price than average, and interestingly, it does not seem like Own Brand products have a lower price than average, which may be because "the use of price promotions for fast moving consumer goods (FMCG’s) by supermarkets has increased substantially over the last decade" (Malik, 2015), particularly for branded products.

```{r}
producttable <- as.data.frame(table(df_ind_summary$product))
producttable <- producttable[order(producttable$Freq, decreasing = TRUE),]
df_ind_summary$product <- factor(df_ind_summary$product, levels = rev(producttable$Var1))

ggplot(df_ind_summary, aes(x= product, y=priceper330ml)) + 
  geom_boxplot(color="#a6bddb") + 
  theme_minimal() + 
  theme(legend.position = "none") + 
  xlab("Product") + ylab("Price (£) per 330ml")  + coord_flip()
```

Figure 2: CSD Data Product Price Ranges

Choice models require the set of available alternatives and prices at the time the shopper made the decision, therefore the CSD data had to be processed to identify which products were available in each store over the time period, based on what other households bought. Ideally this data would be provided in the dataset, as the imputation is not necessarily fully comprehensive, as alternatives may not have been purchased that day. 

The dataset used is missing “feature” and “display” variables that are commonly included and used in this type of analysis (e.g. Zhang, 2006), these are important variables that influence consumer choice. Omitted variable bias may occur if these are not included, as product placement in a prominent place on the store shelf may influence both consumer choice and price sensitivity. The analyst should ensure that these variables are available or that the business is aware of the potential bias in the results.

The model estimates the product share of consumer choice within a category. For this reason the definition of "category" and which products are included within each category is incredibly important, for example: "Should different flavors, or colors, or sizes be treated as different products or lumped together?" (Guadagni & Little, 1983). The preparation of data before running a choice model is therefore highly important. Andrews & Currim (2005) recommend that analysis is done at the lowest level (the SKU) if possible, modelling at the SKU level was popularised by Fader & Hardie (1996). This may well improve predictive performance, but if the business wants analysis at the brand level, then it may be more dangerous to do this aggregation post-modelling, as it is not always theoretically sound to aggregate effects, especially if the model is non-linear, as binary choice models are. The CSD data was analysed at the lowest level of the data provided, except for the "Other", and "Private Label" categories.

Andrews & Currim (2005) also find that, when aggregating, averaging prices causes the price coefficient to be inflated, whereas taking the maximum of a binary promotion variable causes the promotion coefficient to be underestimated. These two findings imply that it makes sense only to aggregate products for which the coefficients are not of primary importance, in this case private label brands and “other” less important brands.

Depending on what technology and infrastructure is being used to run the model, choice model training can also be quite computationally expensive, and limiting the training data may be required in some cases. Andrews & Currim (2005) offer advice on this as well: "selecting purchases of n households over w weeks is slightly preferred over selecting purchases of 2n households over w / 2 weeks", so if the data is to be subset then the analyst should subset over households instead of time where possible.

#Modelling Methodology

The processed dataset then constitutes a set of revealed preferences - the purchases that consumers actually made, as opposed to stated preferences - the products that the consumers explicitly say they like the best. This structure of data allows a multinomial logit model to be run, which became the default after the seminal work of Guadagni & Little (1983). This is an econometric tool that estimates the utility of each product (j) for each consumer (i) $V_{i,j}$ as a linear combination of explanatory variables, either belonging to the household (e.g.  income), or the properties of the product (e.g. price):

$$V_{i,j} = \beta_1 + \beta_2 x_{i} +  \beta_3 x_{i,j} + \dots$$

by modeling the  probability of making a particular choice as per the logit equation, where the probability of consumer i choosing product j is a function of the relative utility:

$$Prob_{i,j} = \frac{e^{V_{i,j}}}{\Sigma_{(k \in K)} e^{V_{i,k}}}$$

The key product level explanatory variables are price and promotion, which are supplemented by household level variables calculated from the data.

The dataset did not already contain household-specific variables such as income or number of children, to be used in the model, despite the fact they are often included in UPC datasets. However, as there was a large enough time window, it was possible to calculate household-level variables from an initialisation period of the initialisation period (the first 60 days). A loyalty variable was calculated: the proportion spent on each product in the initialisation period. An average spend variable was also calculated for each household: the average value spent in the category per week over the first month. Finally, the total purchase incidence was calculated: the number of times the household made purchases in the first month. The data was then limited to households that made some purchases in the initialisation period, to avoid erroneous missing data for these initialisation variables. These calculated variables account for some of the heterogeneity across households.

The final model was then estimated using a multinomial logistic regression model using the mlogit package (Croissant, 2012), and was of the form:

$$V_{i,j} = \alpha_j + \beta_1 price_{i,j}  + \beta_2 loyalty_{i,j} + \beta_3 promotion_{i,j}  + \beta_{4_j} purchase frequency_{i} + \beta_{5_j} averagespend_{i} + \epsilon$$

The intercept is alternative-specific, and represents the "base" utility of the product, relative to the reference product. Price, loyalty and promotion are alternative-specific but have universal coefficients, meaning that there are different prices across products and time, but that consumers have a universal reaction to a change in price, regardless of the product. Purchase frequency and average spend are individual-specific and have alternative-specific coefficients, meaning that there is one, unchanging value per household, but that consumers may have a different relationship with that variable for each product. For example, consumers with a higher average spend may be more likely to purchase premium products, so have positive $beta_{4_j}$ coefficients for premium products and negative $beta_{4_j}$ coefficients for everyday products. The choice of variables in the model, whether they are individual or alternative specific, and whether they have alternative-specific coefficients, is highly important for an analyst to decide, and to understand the implications for calculation of variables and interpretation of coefficients. 

It is also possible to build a predictive choice model with a multinomial choice neutral network (Ripley & Venables, 2016), however these lack interpretability in terms of which factors are contributing to choice, and those factors are the key insights for retail professionals. It is not much use to be able to predict an individual's choice, but rather it is beneficial to understand the mechanisms behind that choice.


```{r}
initialisationperiod <- unique(df_ind_summary$date)[1:60]
df_ind_summary_init <- df_ind_summary[df_ind_summary$date %in% initialisationperiod,]

#get list of households in initialisation period
init_households <- unique(df_ind_summary_init$HOUSE)

#get total purchase incidences
totalpurchases <- data.frame(table(df_ind_summary_init$HOUSE))
colnames(totalpurchases) <- c("HOUSE","purchasefreq")

#get average purchase total
avgspend <- aggregate(NETSPEND ~ HOUSE, data = df_ind_summary_init, mean)
colnames(avgspend) <- c("HOUSE","avgspend")

#get proportion of spend per product
df_ind_summary_init_reshape <- dcast(data = df_ind_summary_init,
                                    formula = HOUSE~product,
                                    fun.aggregate = length,value.var = "NETSPEND")

df_rsums = rowSums(df_ind_summary_init_reshape[,2:length(df_ind_summary_init_reshape)])
df_ind_summary_init_reshape[df_rsums < 5,2:length(df_ind_summary_init_reshape)] <- 0

brandproportions <- cbind(id = df_ind_summary_init_reshape[, 1], df_ind_summary_init_reshape[, -1]/rowSums(df_ind_summary_init_reshape[, -1]))
brandproportions[is.na(brandproportions)] <- 0

colnames(brandproportions) <- paste("loyalty.",gsubfn(".",list(" " = "", "." = "", "(" = "", ")" = ""), colnames(brandproportions)), sep="")
colnames(brandproportions)[1] <- "HOUSE"
```

```{r pricewidecalc, eval=FALSE}
df_ind_summary <- df_ind_summary[!df_ind_summary$date %in% initialisationperiod,]
#Impute prices and availability from data
df_ind_summary$index = 1: nrow(df_ind_summary)
  
#Gets the prices for all products in that shop on that date
findfunction <- function(x, date, shop) {
  if (sum(df_ind_summary$product == x["Var1"] &
                     df_ind_summary$date == date &
                     df_ind_summary$shop == shop) > 0) {return(mean(df_ind_summary$priceper330ml[
                     df_ind_summary$product == x["Var1"] &
                     df_ind_summary$date == date &
                     df_ind_summary$shop == shop]))
                     } else {return(0)}
}

f <- function(x, output) {
    print(as.numeric(x["index"])/nrow(df_ind_summary))
    return(apply(producttable,1, findfunction, x["date"], x["shop_desc"]))
 }

df_ind_summary_wide <- apply(df_ind_summary, 1, f)
rownames(df_ind_summary_wide) <- producttable$Var1
write.csv(df_ind_summary_wide,"df_ind_summary_wide.csv")
```

```{r promowidecalc, eval=FALSE}
df_ind_summary <- df_ind_summary[!df_ind_summary$date %in% initialisationperiod, ]
df_ind_summary$promotion <- 1
df_ind_summary$promotion[df_ind_summary$eprom %in% c("No Promotion"," No Promotion (Price Marked)")] <- 0

#Impute prices and availability from data
df_ind_summary$index = 1:nrow(df_ind_summary)
  
#Gets the prices for all products in that shop on that date
findfunction <- function(x, date, shop) {
  if (sum(df_ind_summary$product == x["Var1"] &
                     df_ind_summary$date == date &
                     df_ind_summary$shop == shop) > 0) {return(max(df_ind_summary$promotion[
                     df_ind_summary$product == x["Var1"] &
                     df_ind_summary$date == date &
                     df_ind_summary$shop == shop]))
                     } else {return(0)}
}

f <- function(x, output) {
    print(as.numeric(x["index"])/119742)
    return(apply(producttable,1, findfunction, x["date"], x["shop_desc"]))
 }

df_ind_summary_wide_prom <- apply(df_ind_summary, 1, f)
rownames(df_ind_summary_wide_prom) <- producttable$Var1
write.csv(df_ind_summary_wide_prom,"df_ind_summary_wide_prom.csv")
```

```{r loadwidedata, eval=FALSE}
df_ind_summary_wide_prom <- read.csv("df_ind_summary_wide_prom.csv")
df_ind_summary_wide_prom_matrix <- data.matrix(df_ind_summary_wide_prom[2:dim(df_ind_summary_wide_prom)[2]])

df_ind_summary_wide <- read.csv("df_ind_summary_wide.csv")
df_ind_summary_wide_matrix <- data.matrix(df_ind_summary_wide[2:dim(df_ind_summary_wide)[2]])
```

```{r processwidedata, eval=FALSE}
#Make promotion dataframe by transposing and adding "prom." to column names
df_ind_summary_wide_prom_t <- data.frame(t(df_ind_summary_wide_prom_matrix))

colnames(df_ind_summary_wide_prom_t) <- paste("prom.", gsubfn(".",
                                             list(" " = "", "." = "", "(" = "", ")" = ""),
                                             as.character(df_ind_summary_wide_prom$X)),
                                             sep="")

#Make price dataframe by transposing and adding "price." to column names
df_ind_summary_wide_t <- data.frame(t(df_ind_summary_wide_matrix))
names(df_ind_summary_wide_t) <- paste("price.", gsubfn(".",
                                             list(" " = "", "." = "", "(" = "", ")" = ""),
                                             as.character(df_ind_summary_wide[,1])), sep="")
#Make availability dataframe by checking if price = 0 and adding ".availability" to col names
availability <- data.frame(apply(df_ind_summary_wide_matrix, 1, function(x) {x != 0}))
names(availability) <- paste("availability.",gsubfn(".",
                                    list(" " = "", "." = "", "(" = "", ")" = ""),
                                    as.character(df_ind_summary_wide[,1])), sep="")
##concatenate required fields for mlogit
df_ind_summary_extended <- cbind(df_ind_summary[,c("date","shop_desc","product","HOUSE")],
                                 df_ind_summary_wide_t,availability, df_ind_summary_wide_prom_t)

##136 df_ind_summary_wide_t
##134 df_ind_summary_wide_prom_t

df_ind_summary_extended$product <- gsubfn(".",
                                          list(" " = "", "." = "", "(" = "", ")" = ""),
                                          as.character(df_ind_summary_extended$product))
df_ind_summary_extended <- df_ind_summary_extended[!df_ind_summary_extended$date %in% initialisationperiod,]
df_ind_summary_extended <- df_ind_summary_extended[df_ind_summary_extended$HOUSE %in% init_households,]
df_ind_summary_extended <- merge(df_ind_summary_extended, totalpurchases)
df_ind_summary_extended <- merge(df_ind_summary_extended, avgspend)
df_ind_summary_extended <- merge(df_ind_summary_extended, brandproportions)

save(df_ind_summary_extended, file="df_ind_summary_extended.RData")
```

```{r loadextendeddata}
load("df_ind_summary_extended.RData")
```

```{r converttomlogitdata, eval=FALSE}
#takes 1 min to run
#create data frame with all data, including availability (in wide format)
temp_mlogit <- mlogit.data(df_ind_summary_extended,
                           shape = "wide",
                           varying = c(price=5:24,
                                       availability=25:44,
                                       prom=45:64,
                                       loyalty=67:86),
                           choice = "product", id.var = "HOUSE")
#subset only to the products that are available
temp_available <-  temp_mlogit[temp_mlogit$availability ,] 
temp_available <- temp_available[ , !(names(temp_available) %in% "availability")]

#and resend it through mlogit.data with format long 
csds.data <- mlogit.data(temp_available , choice = 'product', shape = "long" , alt.var = "alt" ,  chid.var = "chid")

save(csds.data, file="csdsdata14.RData")
```

```{r toremove, eval=FALSE}
#takes 1 min to run
#create data frame with all data, including availability (in wide format)
temp_mlogit <- mlogit.data(df_ind_summary_extended[df_ind_summary_extended$HOUSE %in% 
                                                     c( #7up
                                                       720868,170463,
                                                       #DC
                                                       741317,31969),
                                                   c("HOUSE","date","product",
                                                     "price.DietCokeDiet", "price.Other",
                                                     "price.TangoRegular", "price.DrPepperRegular",
                                                     "price.7UpRegular", "price.RelentlessRegular",
                                                     "price.FantaDiet", "price.FantaRegular",
                                                     "price.RedBullRegular",
                                                     "availability.DietCokeDiet", "availability.Other",
                                                     "availability.TangoRegular", "availability.DrPepperRegular",
                                                     "availability.7UpRegular", "availability.RelentlessRegular",
                                                     "availability.FantaDiet", "availability.FantaRegular",
                                                     "availability.RedBullRegular",
                                                     "loyalty.DietCokeDiet", "loyalty.Other",
                                                     "loyalty.TangoRegular", "loyalty.DrPepperRegular",
                                                     "loyalty.7UpRegular", "loyalty.RelentlessRegular",
                                                     "loyalty.FantaDiet", "loyalty.FantaRegular",
                                                     "loyalty.RedBullRegular",
                                                     "purchasefreq","avgspend")],
                           shape = "wide",
                           varying = c(price=4:12,
                                       availability=13:21,
                                       loyalty=22:30),
                           choice = "product", id.var = "HOUSE")
#subset only to the products that are available
temp_available <-  temp_mlogit[temp_mlogit$availability ,] 
temp_available <- temp_available[ , !(names(temp_available) %in% "availability")]

#and resend it through mlogit.data with format long 
csds.data <- mlogit.data(temp_available , choice = 'product', shape = "long" , alt.var = "alt" ,  chid.var = "chid")

save(csds.data, file="csdsdata9.RData")
```

```{r runmodel, eval=FALSE}
#takes 1h30 to run
Sys.time()
csds.ml14 <- mlogit(product ~ price + loyalty + prom | avgspend + purchasefreq, csds.data)
Sys.time()
summary(csds.ml14)

save(csds.ml14, file="csdsml14.RData")

#takes 1h30 to run
Sys.time()
csds.ml15 <- mlogit(product ~ price + loyalty + prom, csds.data)
Sys.time()
summary(csds.ml15)

save(csds.ml15, file="csdsml15.RData")
```

#Results and Visualisation

```{r shareofchoice}
load("csdsml14.RData")
csds.ml <- csds.ml14

choiceshare <- as.data.frame(round(summary(csds.ml)$freq/sum(summary(csds.ml)$freq)*100,2))
colnames(choiceshare) <- c("Product","Share of Choice (%)")

shareofvolume <- aggregate(NETSPEND ~ product, FUN=sum, data=df_ind_summary)
shareofvolume$NETSPEND <- round(shareofvolume$NETSPEND/sum(shareofvolume$NETSPEND)*100,2)
colnames(shareofvolume) <- c("Product","Share of Revenue (%)")
shareofvolume$Product <- gsubfn(".",
                                    list(" " = "", "." = "", "(" = "", ")" = ""),
                                    as.character(shareofvolume$Product))

sharetable <- merge(choiceshare, shareofvolume) 
sharetablesorted <- sharetable[order(sharetable$'Share of Choice (%)', decreasing = TRUE),]
```

Panel scanner data allowed manufacturers and retailers to see not just product share of sales volume or revenue, but also their share of choice (Kamakura & Russel, 1989). Share of choice represents the amount of times consumers choose to buy a product over all others, regardless of the quantity that they buy or how much they spend. This is particularly important for manufacturers, who often want to understand how a product is performing within a category, regardless of the size of the category. Shares of choice and volume of the 25 products can be seen in Appendix A. The largest product, Diet Coke, has a higher share of revenue (£) than of choice, implying that consumers have a high purchase quantity for this product.

The output of the trained model can be found in Appendix B. The coefficient on price is negative and statistically significant, showing that consumers are exhibiting usual behaviour of switching away from products with high prices and towards those with lower prices. The coefficient on loyalty is positive and statistically significant, which aligns with the expectation that consumers are more likely to purchase a product that they often purchased in the initialisation period. This coefficient is important for manufacturers to understand about their category, if this coefficient is high, or is increasing over time, then this indicates that product loyalty has a strong impact (or is growing in impact), so more focus should be placed on improving brand image, or on engaging consumers early in their lifecycle. If the coefficient is negative, or reducing, then this could indicate that consumers are exhibiting variety-seeking behaviour (Seetharaman & Chintagunta, 1998), and marketing activity for manufacturers with varied products may want to capitalise on that. The coefficient on promotion is also positive and statistically significant, as per expectations, implying that consumers respond to promotional offers by switching to a promoted product.

The coefficient for the intercept of Diet Coke is positive, showing that consumers place higher utility on this product as compared to the reference product (7upDiet). Diet Coke also has a positive coefficient for average spend, meaning that consumers that spent more on average in the initialisation period tend to choose Diet Coke more than those who spent less, so it could be identified as a "premium" product when compared to Old Jamaica which has a negative coefficient. The purchase quantity coefficient for Diet Coke is positive, implying that regular shoppers buy it, again as opposed to Old Jamaica which has a negative coefficient, so seems to be more popular for infrequent shoppers. Each of these are statistically significant effects as estimated by the model. Similar analysis can be done for all products by analysing the model output results.

Product share own price elasticity and cross price elasticity can then be calculated from the price coefficient by calculating the marginal effects and dividing by the percentage price change (Calculation details can be found in Appendix C). 

```{r elast}
load("csdsdata14.RData")

##Calculate Elasticity Matrix
elastdf <- setNames(data.frame(matrix(ncol = length(unique(csds.data$alt)), nrow = 0)), unique(csds.data$alt))
for (product1 in unique(csds.data$alt)){
  elast <- vector()
  for (product2 in unique(csds.data$alt)){
    #Calculate own share elasticity
    if (product1 == product2) {
      elast.temp <- mean(csds.ml$coefficients["price"]*csds.ml$probabilities[,product1]*(1-csds.ml$probabilities[,product1])) /
        mean(csds.ml$probabilities[,product1]) * mean(csds.data$price[csds.data$alt==product1]) } else {
    #Calculate cross share elasticity
      elast.temp <- -1*mean(csds.ml$coefficients["price"]*csds.ml$probabilities[,product1]*csds.ml$probabilities[,product2]) /
        mean(csds.ml$probabilities[,product1]) * mean(csds.data$price[csds.data$alt==product2]) }
    elast <- c(elast,round(elast.temp,4))}
  elastdf[product1,] <- elast}
```

The full set of elasticities and cross price elasticities are reported in Appendix D, importantly all cross price elasticities are positive, and own price elasticities are negative, indicating that when the price of a product goes up, consumers are less likely to buy that product, and more likely to buy other products, so the landscape of CSDs is one made up of competitors. Diet Pepsi and Diet Coke have higher than average cross-price elasticities with each other, confirming that they are strong competitor products. This information is useful for manufacturers to decide on pricing strategies, in particular to assess the effectiveness of Every Day Low Prices (EDLP) strategies compared to promotional strategies (Shankar & Krishnamurthi, 1996). At a more operational level, product share price elasticity information is also useful for manufacturing company account managers and retailer buyers, who work together regularly to define short term promotional prices.

Once estimates of price elasticities and cross-price elasticities have been derived, the competitive landscape can then be described using clout and vulnerability analysis, where clout is the sum of the outbound cross price elasticities - representing how much the product’s price affects purchases of other products, and vulnerability is the sum of the inbound cross price elasticities - representing how much the product’s sales is dependent on other products’ prices (Kamakura & Russel, 1989). The clout and vulnerability for the produts in the CSD dataset are displayed below, where the size of the node represents the share of choice:

```{r fig.width=8}
elastdf_cv <- elastdf
elastdf_cv[row(elastdf_cv) == (col(elastdf_cv))] <- 0

clout <- as.data.frame(colMeans(elastdf_cv))
colnames(clout) <- "clout"
vuln <- as.data.frame(rowMeans(elastdf_cv))
colnames(vuln) <- "vulnerability"

cvdf <- cbind(clout,vuln, sharetable[,2])
colnames(cvdf)[3] <- "share"

cvdf$product<-rownames(cvdf)


ggplot(cvdf, aes(x=vulnerability, y=clout, size=share)) + 
  geom_point(color="#a6bddb") +
  geom_text(data=subset(cvdf, product %in% c("CocaColaRegular","Other","DietCokeDiet",
               "OwnBrandDiet","OwnBrandRegular","PepsiRegular", "MonsterRegular",
               "PepsiDiet","RedBullRegular","CocaColaZeroDiet")),
            aes(vulnerability,clout,label=product), size=3, col="#383838") + 
  theme_bw() + theme(legend.position = "none")
```

Unlabeled: 7UpDiet, 7UpRegular , CocaColaLifeRegular , DrPepperRegular, FantaRegular, OldJamaicaRegular , RelentlessRegular, RubiconRegular, TangoRegular, FantaDiet.

Products from strong brands such as Coke and Pepsi have relatively high clout and lower vulnerability, whereas the more niche products are clustered together with high vulnerability and low clout. This is to be expected as consumers are unlikely to switch from strong brands to niche products as a result of a price change from the niche product, but they are likely to change their choice due to price changes from a strong brand. It is interesting that in this dataset Regular Pepsi has low clout and high vulnerability even though it has a high share of choice. This may mean that it is a common "second choice alternative" to the other strong brands when they change to have a higher price. 

This clout and vulnerability analysis can be useful for brands to understand where they stand in the competitive landscape. This landscape is likely to change over time, as consumer preferences change, or when new products enter the market (Van Heerde, Mela & Manchanda, 2004). Therefore it is important for an analyst to regularly repeat this procedure, and show the changes over time. This can highlight which products are starting to compete more heavily with each other, for example that SUVs used to compete more strongly with vans, then grew to compete more competitively with cars (Luan, Sudhir & Norris, 2007).   

Panel scanner data also allowed businesses to understand how promotional activity affected consumers at an individual level (Van Heerde, Leeflang & Wittick, 2004). Using the promotion coefficient, clout and vulnerability analysis can also be done on a promotional / advertising basis, to understand how sales are impacted by the promotions of other products. 

#Vertical Extension of the Choice Model


```{r, echo = FALSE, fig.pos = 'h', fig.align='center'}
knitr::include_graphics("mldt.png")
```

*Figure 2 - Brand Choice Decision Tree, as per structure of Sismeiro (2017).*

The multinomial logit model of brand choice, as per Figure 2, assumes that the consumer is making a purchase within the category. It focuses on brand choice, and ignores other key business questions such as: what are the drivers for how often consumers come to the shop (purchase incidence), and how much do they buy when they are there (purchase quantity). As mentioned in Guadagni & Little (1983), without these questions being answered the analysis provides limited value for manufacturers, who are interested not only in brand share but in purchase incidence and quantity in the category, and even more limited value to the retailer, who is interested in total spend at their store. The choice model can be extended vertically to provide answers to these questions.

```{r, echo = FALSE, fig.pos = 'h', fig.align='center'}
knitr::include_graphics("mldt_pi.png")
```

*Figure 3 - Brand Choice and Purchase Incidence Decision Tree, as per structure of Sismeiro (2017).*

One of the commonly used modelling techniques to cater for purchase incidence, as per Figure 3, is the nested logit model (Guadagni & Little, 1998). This model makes a broader assumption that the consumer is making a shopping trip, and then models whether the consumer will make a purchase within the category, and subsequently what their product choice would be. The nested approach first estimates the traditional multinomial logit, and then uses the concept of "category attractiveness", to add the additional layer of product category choice on top of this. This approach can be built to handle the case where multiple different products are purchased in one shopping trip, as it can cater for multiple purchase opportunities within one shopping trip. 

```{r, echo = FALSE, fig.pos = 'h', fig.align='center'}
knitr::include_graphics("mldt_pipq.png")
```

*Figure 4 - Brand Choice, Purchase Incidence and Purchase Quantity Tree, as per structure of Sismeiro (2017).*

This can then further be extended to purchase quantity, as per Figure 4, as has been done by (Gupta, 1998; Chiang 1991; Chintagunta 1993; Bucklin, Gupta & Siddarth, 1998). For this, a zero-truncated Poisson model is often used (e.g. in Bucklin, Gupta & Siddarth, 1998). This approach models count data when it is known that the count is greater than 0, which is applicable in this situation as this estimation occurs after we know the consumer has made a purchase. The quantity elasticity that is produced by this model can be added to the brand choice elasticity from the multinomial logit model to get the overall elasticity.

The interaction between purchase quantity price and promotion elasticities and their product switching counterparts is important for the discussion between retailers and manufacturers. Manufacturers care about increasing the overall sales volume of their brand, so purchase quantity and brand switching elasticities should be aggregated to gauge the full effect, but the retailer doesn’t benefit from consumer product switching, as they lose the revenue from the products consumers are switching from (Srinivasan et al., 2004). It is important for both parties to have good estimates of both the elasticities in order to make their decisions properly, even though the corresponding actions are likely to differ. 

```{r, echo = FALSE, fig.pos = 'h', fig.align='center'}
knitr::include_graphics("mldt_pipqrc.png")
```

*Figure 5 - Brand Choice, Purchase Incidence, Purchase Quantity and Retailer Choice Tree, as per structure of Sismeiro (2017).*

An additional layer could also be added to model, the choice of retailer e.g. by Bucklin & Lattin (1992) as per Figure 5. 

#Horizontal Extension of the Choice Model

Much research has gone into using UPC scanner data to produce models for consumer purchasing decisions, and many additional subtleties and complexities that can be incorporated into the models have been identified to cater to more nuanced consumer behaviour. This section outlines some of the ways that choice models can be extended, that an analyst should consider.

There is evidence that consumers do not always respond directly to price and instead compare prices to a “reference price" - how much they think the item should cost (Winer, 1986). There are of course difficulties with modelling this internal psychological price, but proxies for it can be made by using average price for comparable products on that day, the price last paid for the product by that consumer, or the average price for the product. There may also be an additional mechanism where consumers respond differently to a price above the reference point than to a price below (Krishnamurthi, Mazumdar & Raj, 1992), following the “loss aversion” research of Kahneman and Tversky (1991).

Consumers may also have internal price “thresholds” and change their choice only if a competitor brand is below a threshold, or their usual brand is above a threshold. These thresholds could also be relative to a dynamic reference price. These thresholds can be estimated for consumers or consumer segments, and then included into the model (Han, Gupta & Lehmann, 2001). 

There is evidence that suggests that the price elasticity may also change across the life-cycle of the product (Simon 1979, Parker, 1992). Bijmolt, Van Heerde and Pieters (2005) also found that overall price elasticities in general have been increasing over time, whereas share elasticities have remained quite constant. Elasticities could also be dependent on the particular store or retailer (Hoch et al 1995). How these mechanisms relate to each other has key business implications, such as how to structure a pricing strategy over a product lifecycle (Bijmolt, Van Heerde and Pieters, 2005), and how to manage promotional pricing across retailers. 

Additional loyalty variables could also be calculated in addition to the product-loyalty calculated in the CSD example. This could include higher-level range or brand loyalty (e.g Pepsi / Coca Cola), package-size loyalty (Guadagni & Little, 1983) and flavour loyalty, to identify consumer reactions to specific product attributes.

Some modelling approaches also include an inventory variable (e.g. Chintagunta 1993) by estimating a consumer’s consumption rate during the initialisation period, then estimating their “inventory” of the product by adding to the inventory variable every time they make a purchase, and making reductions from it at the calculated rate of consumption.

Walsh (1995) also suggests an alternative model for cases in which there are often various different products purchased on each shopping occasion. This alternative model focuses on the "assortment" of goods that a consumer has in their inventory, and takes into account that the consumer purchase goods for unknown future occasions and may therefore value variety. Dubé (2004) offers yet another model to handle this case.

Consumer segments can also be created based on choice behaviour. Often in the literature consumers are split up into “loyal” and “switching” consumers (Kamakura & Russel, 1989). Clout and vulnerability analysis can also be repeated on consumer segments, to understand how the competitive structure differs, and can be useful for developing segment-targeted marketing and product strategies. Bucklin & Gupta (1992) added another refinement to the model by including household response segments, grouping households by how they react to price and promotion variables, and then estimating their responses as a group. This caters for some of the heterogeneity amongst consumers, and by maintaining groups instead of going to the individual level, the reported segment variables are "managerially interpretable" Guadagni & Little (1998). Bucklin, Gupta & Siddarth (1998) continue this segmentation approach across the three tiers of purchase incidence, brand choice, and purchase quantity, as per Figure 4.

The model could be further extended by splitting the base price and the promotional price, or between different types of promotions. In this case that would require using a categorisation of promotion descriptions either with a look-up table, using NLP techniques on promotion descriptions, or by comparing average non-promotional prices to promotional prices. Choice models could also be extended to include multiple "states" of consumer behaviour, e.g. planned purchases and spontaneous purchases. (Bucklin and Little, 1991). 

Each of these additional elements would lead to a more refined model, catering for additional consumer considerations and behaviours, unfortunately in most cases they also lead to additional computational complexity, and if care is not taken, to results that are less easy to interpret by a retail professional. If one of the purposes of the analysis is to understand pricing strategies then the inclusion of the more nuanced consumer price response behaviours would provide a much more useful model than the price elasticity of the “basic” model. However, if the purpose is to undercover product attribute importance or promotional effects, then it may not be worth the additional modelling effort to establish the reference and threshold points, only to increase the predictive accuracy of the model.

#Additional Data Sources

The agencies that produce these datasets also run econometric models and provide reports, analysis and consultancy to businesses (e.g. Neilsen, 2017). Therefore, for a data scientist beginning to explore this data for a business, it is worth first understanding what types of reporting and analysis that business is already receiving from the data provider or other agencies. If a data scientist is charged with getting insights from this data, it may be because the business:

* Is not willing to pay for the pre-packaged results from the data provider, in which case the above techniques apply.
* Wants a custom model including variables or behaviours that are specific to their industry, in which case the above techniques and the model extension discussion applies, and there may well be additional modelling steps.
* Wants to merge the data with another dataset, to get additional insights.

This section focuses on the last reason, in particular the new datasets that are available for business to access, and how they link to customer choice data.

Consumer choice models could also be merged with GDP growth or inflation data to include economic conditions, which may impact consumer price elasticity and choice behaviour. For the purchase quantity model it may also be useful to include local weather data, but this is less likely to be useful for the brand choice model. Weather isn’t clearly linked to consumers’ price or promotion elasticity, so although inclusion may improve the prediction rate, omission is unlikely to cause bias. In more recent years in-store panel data has also been linked to online sales data, to model consumers who use both channels, or analyse how it differs across those channels. There are many elements that may differ including price elasticity (Degeratu, Rangaswamy and Wu, 2000) and brand loyalty (Danaher, Wilson and Davis, 2003).

There has been an explosion over the last decade in data that consumers provide freely relating to their shopping habits, their brand preferences, and their decision making behaviour. A major force driving this is the increase in usage of social media sites such as twitter where consumers post publically about their daily experiences. Social media “exposure” to a product, could be measured and included into the choice model as a brand specific attribute that changes over time (Liu & Lopez, 2016). This "exposure" could measure social media advertising, but also organic conversation about the brand. This could also be further broken down into positive or negative exposure based on sentiment analysis of the social media content, or exposure to nutritional social media discussions such as sugar and caffeine (as in Liu & Lopez, 2016), or to ingredients, flavours or other product attributes. 

Social media data could also be used to complement the outcomes of the statistical analysis. If the model shows a high absolute brand choice price elasticity to a competitor brand, then qualitative analysis of social media data and the conversations that consumers are having when they mention the two brands, could identify what features consumers see as important. Data science techniques such as word embedding (Golberg & Levy, 2014) could be used here to identify language that corresponds to brand-switching, in order to pick out relevant blogs or tweets to analyse.

There are clearly some limitations of using social media in this type of analysis, and monitoring sentiment on social media does not always lead to robust predictions. Consumers that regularly access social media sites are a growing proportion across age groups, but social media exposure is likely to be more impactful for specific age groups (Correa, Hinsley and De Zuniga, 2010). For an analyst looking at data from a specific retailer or manufacturer, additional data sources should be analysed for the specific industry and category. Twitter data may be more applicable in a Business-to-Consumer context than a Business-to-Business context, for example, particularly for a business whose target consumers are from younger age groups. It seems to be likely that social data will be increasingly used for retail analytics in all fields over the coming years.

#Further Choice Model Applications

Choice models can also be used outside of the retail space, for example to model how people choose: a profession (Boskin, 1974), a house and mode of commute (Pinjari et al, 2011), where to attend college (Long, 2004), choice of vehicle (Berkovec, 1985). The important considerations to take when deciding whether to use a choice model include:

* Whether the analyst has access to the information of available alternatives (e.g. what other cars are on the market)
*	Whether a high percentage of the variables are available on which the individuals are expected to make their decisions (e.g. the price, number of doors)
* Whether the relevant decision making behaviours can be modelled or proxied sufficiently (e.g. whether there is information on the existence of, or values for reference prices and thresholds in the situation)
* Whether there are enough individuals, with data for multiple choices over time, to make the analysis statistically significant

#Future Business Usage

Evidence has shown that marketing executives often make price promotion decisions intuitively rather than relying on the outputs of the empirical methods discussed above (Bogomolova, Szabo & Kennedy, 2017). Many of the theoretical and academic approaches are highly complex, have complex outcomes, and can use esoteric language. These two facts are very likely linked. The outcomes of retail modelling approaches must be clear and actionable, and ideally the methodology used to achieve those outcomes must be understandable (Neslin et al., 1994). There is empirical evidence that analysing consumer data has a positive impact on company financial performance (Germann et al., 2014). It is therefore the difficult but critical job of the analyst to ensure that the outputs are meaningful, using the clout and visualisation framework may help here, or displaying visual simulations of changes to the marketing mix.

Before the introduction of panel scanner data, the modelling of consumer choice had been largely dominated by psychological and sociological theory. The future of consumer choice research is likely be data-driven, rooted in the known principles of psychology, but also coherent to retail professionals. The academic fields are already starting to form a symbiotic relationship where each informs the other to build strong and predictive models of consumer choice, and professional agencies are working on bringing those insights to retail professionals. As the field progresses, these links will likely tighten, and additional data sources will be added more and more from social media and other modern platforms.

Consumer choice models are likely to continue to be used to estimate elasticities and understand in-store consumer behaviour. As mentioned, there have been extensions to online behaviour, but choice models must also find their place alongside algorithms that are becoming common in the industry such as real-time pricing algorithms, next best offer or action models, and website analytics. Businesses across retail and many other industries are becoming more algorithm focused. A clear understanding of the business value, and applicable use cases for these aglorithms will be increasingly important for any analyst. 

#Conclusion

Consumer choice models, as illustrated by the CSD example in this report, are powerful tools that can be highly beneficial to both retailers and manufacturers while deciding on the elements of the marketing mix. However, care must be taken that the models are constructed meaningfully, and explained correctly and intuitively to retail professionals. As in all statistical enterprises, the quality of the model is limited by the quality and quantity of the input data. The analyst should strive as much as possible to have a full set of variables, calculate additional variables where appropriate, and use additional data sources if and only if they are likely to add value. There are many decisions that should be taken when producing a model of consumer choice, and close attention should be paid to the impact of those decisions, from data preprocessing, to model specification, and meaningful visualisation of results.

#References

Andrews, R.L. and Currim, I.S., (2005) An experimental investigation of scanner data preparation strategies for consumer choice models. *International Journal of Research in Marketing*. 22(3), 319-331.

Bijmolt, T.H., Van Heerde, H.J. and Pieters, R.G. (2005) New empirical generalizations on the determinants of price elasticity. *Journal of Marketing Research*. 42(2), 141-156.

Bogomolova, S., Szabo, M. and Kennedy, R., (2017) Retailers' and manufacturers' price-promotion decisions: Intuitive or evidence-based?. *Journal of Business Research*. 76, 189-200.

Berkovec, J. and Rust, J. (1985) A nested logit model of automobile holdings for one vehicle households. *Transportation Research Part B: Methodological*. 19(4), 275-285.

Boskin, M.J. (1974) A Conditional Logit Model of Occupational Choice. *Journal of Political Economy*. 82(2, Part 1), 389–398.

Bucklin, R.E. and Gupta, S. (1999) Commercial Use of UPC Scanner Data: Industry and Academic Perspectives. *Marketing Science*. 18(3), 247–273.

Bucklin, R.E., Gupta, S. and Siddarth, S. (1998) Determining segmentation in sales response across consumer purchase behaviors. *Journal of Marketing Research*. 189-197.

Bucklin, R.E. and Lattin, J.M.(1991) A two-state model of purchase incidence and brand choice. *Marketing Science*. 10(1), 24-39.

Bucklin, R.E. and Lattin, J.M., (1992) A model of product category competition among grocery retailers. *Journal of Retailing*. 68(3), 271.

Chintagunta, P.K., (1993) Investigating Purchase Incidence, Brand Choice and Purchase Quantity Decisions of Households. *Marketing Science*. 12(2), 184–208.

Correa, T., Hinsley, A.W. and De Zuniga, H.G. (2010) Who interacts on the Web?: The intersection of users’ personality and social media use. *Computers in Human Behavior*. 26(2), 247-253.

Croissant, Y. (2012) *Estimation of multinomial logit models in R: The mlogit Packages. R package version 0.2-2*. Available from: http://cran.r-project.org/web/packages/mlogit/vignettes/mlogit.pdf [Accessed August 24th 2017]

Ripley, B., Venables, W. (2016) *Package ‘nnet’. R package version*. Available from: https://cran.r-project.org/web/packages/nnet/nnet.pdf [Accessed August 24th 2017]

Danaher, P.J., Wilson, I.W. and Davis, R.A. (2003) A comparison of online and offline consumer brand loyalty. *Marketing Science*. 22(4), 461-476.

Degeratu, A.M., Rangaswamy, A. and Wu, J. (2000) Consumer choice behavior in online and traditional supermarkets: The effects of brand name, price, and other search attributes. *International Journal of research in Marketing*. 17(1), 55-78.

Dubé, J.P., (2004) Multiple discreteness and product differentiation: Demand for carbonated soft drinks. *Marketing Science*. 23(1), 66-81.

Fader, P.S. and Hardie, B.G. (1996) Modeling consumer choice among SKUs. *Journal of marketing Research*. 442-452.

Germann et al. (2014) Do Retailers Benefit from Deploying Customer Analytics? *Journal of Retailing*. 90(4), 587–593.

Google Ngrams, (2008) *Representation of frequency of the terms "marketing science" and "UPC Scanner" in the Google ngram corpus from 1970 to 2008, digital image of data, Google Ngrams* Available from: https://books.google.com/ngrams/graph?content=UPC+scanner%2Cmarketing+science&year_start=1970&year_end=2008&corpus=15&smoothing=2&share= [Accessed 10th August 2017]

Guadagni, P.M. and Little, J.D. (1983) A Logit Model of Brand Choice Calibrated on Scanner Data. *Marketing Science*, 2(3). 203–238.

Guadagni, P.M. and Little, J.D. (1998) When and what to buy: a nested logit model of coffee purchase. *Journal of Forecasting*. 17(3‐4), 303–326.

Guadagni, P.M. and Little, J.D. (2008) A Logit Model of Brand Choice Calibrated on Scanner Data: A 25th Anniversary Perspective. *Marketing Science*. 27(1), 26–28.

Gupta, S. (1988) Impact of Sales Promotions on When, What, and How Much to Buy. *Journal of Marketing Research*. 25(4), 342–355.

Goldberg, Y. and Levy, O. (2014) *word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method*. Available from: https://arxiv.org/abs/1402.3722 [Accessed 24th August 2017]

Han, S., Gupta, S. and Lehmann, D.R. (2001) Consumer price sensitivity and price thresholds. *Journal of retailing*. 77(4), 435-456.

Hoch, S. et al. (1995) Determinants of Store-Level Price Elasticity. *Journal of Marketing Research* 32(1), 17.

Tversky, A. and Kahneman, D. (1991) Loss aversion in riskless choice: A reference-dependent model. *The Quarterly Journal of Economics*. 106(4), 1039-1061.

Kamakura, W.A. and Russell, G. (1989) A probabilistic choice model for market segmentation and elasticity structure. *Journal of Marketing Research*. 24, 379-390.

Krishnamurthi, L., Mazumdar, T. & Raj, S.P. (1992) Asymmetric Response to Price in Consumer Brand Choice and Purchase Quantity Decisions. *Journal of Consumer Research*. 19(3), 387–400.

Liu, Y. and Lopez, R.A. (2016) The impact of social media conversations on consumer brand choices. *Marketing Letters,* 27(1), 1-13.

Long, B.T. (2004) How have college decisions changed over time? An application of the conditional logistic choice model. *Journal of Econometrics*. 121(1), 271-296.

Luan, J.Y., Sudhir, K. and Norris, B. (2007) *Dynamic market structure in a durable goods market: The effect of a new product form*. Yale School of Management. Available from: http://faculty.som.yale.edu/ksudhir/workingpapers/MktStr-JMR.pdf [Accessed  21st August 2017]

Malik, S.A. (2015) *Optimising supermarket promotions of fast moving consumer goods using disaggregated sales data: A case study of Tesco and their small and medium sized suppliers*. Doctoral dissertation, University of Kent.

Neslin et al. (1994) A research agenda for making scanner data more useful to managers. *Marketing Letters* 5(4), 395–411

Neilsen (2017) *Consumer Insight Solutions* Available from: http://www.nielsen.com/uk/en/solutions/consumer-insights.html [Accessed 24th August 2017]

Parker, P. (1992) Price Elasticity Dynamics Over the Adoption Life Cycle. *Journal of Marketing Research*. 29(3), 358–367.

Pinjari, A. et al. (2011) Modeling the choice continuum: an integrated model of residential location, auto ownership, bicycle ownership, and commute tour mode choice decisions. *Transportation*. 38(6), 933–958.

Seetharaman, P.B. and Chintagunta, P. (1998) A model of inertia and variety-seeking with marketing variables. *International Journal of Research in Marketing* 15(1), 1-17.

Simon, H. (1979) Dynamics of Price Elasticity and Brand Life Cycles: An Empirical Study. *Journal of Marketing Research*. 16(4), p.439.

Sismeiro, C., (2017). *Retail Analytics* [Lecture] Msc Business Analytics, Imperial College London.

Shankar V. & Krishnamurthi, L. (1996) Relating price sensitivity to retailer promotional variables and pricing policy: an empirical analysis. *Journal of Retailing*. 72(3), 249–272.

Srinivasan, S., Pauwels, K., Hanssens, D.M. and Dekimpe, M.G. (2004) Do promotions benefit manufacturers, retailers, or both? *Management Science*. 50(5), 617-629.

Van Heerde, H.J., Leeflang, P.S. and Wittink, D.R. (2004) Decomposing the sales promotion bump with store data. *Marketing Science*. 23(3), 317-334.

Van Heerde, H.J., Mela, C.F. and Manchanda, P. (2004) The dynamic effect of innovation on market structure. *Journal of Marketing Research*. 41(2), 166-183.

Walsh, J.W., (1995) Flexibility in consumer purchasing for uncertain future tastes. *Marketing Science*. 14(2), 148-165.

Winer, R.S. (1986) A reference price model of brand choice for frequently purchased products. *Journal of Consumer Research* 13(2), 250-256.

Zhang, J. (2006) An Integrated Choice Model Incorporating Alternative Mechanisms for Consumers; Reactions to In-Store Display and Feature Advertising. *Marketing Science*. 25(3), 278–290.

\newpage

##Appendix A

```{r appA}
kable(sharetablesorted, row.names = FALSE)
```

\newpage

##Appendix B

###Universal Coefficients

```{r appB1, cache=FALSE}
tabletoshow <- mlogittotable(csds.ml, keys=rownames(summary(csds.ml)$CoefTable)[20:22])
tabletoshow$variables <- as.character(tabletoshow$variables)
tabletoshow$variables <- gsubfn(".", list(":" = " - "), tabletoshow$variables)
kable(tabletoshow)
```

Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1

###Product Specific Coefficients

```{r appB2, cache=FALSE}
alt_keys <- rownames(summary(csds.ml)$CoefTable)[c(1:10,23:60)]
alt_keys <- sort(alt_keys)
tabletoshow <- mlogittotable(csds.ml, keys=alt_keys)
tabletoshow$variables <- as.character(tabletoshow$variables)
tabletoshow$variables <- gsubfn(".", list(":" = " - "), tabletoshow$variables)
kable(tabletoshow)
```

Signif. codes:  0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1


\newpage

##Appendix C - Elasticity Calculation Method

Elaticities are calculated using the formulas below:

Own Price Elasticity of product i = 
$$\frac{\beta_{price} 1/n\sum{P_{i,h,t}(1-P_{i,h,t}}) \overline{price_{i}}}{1/n\sum{P_{i,h,t}}}$$

Cross Price Elasticity of product i to price j = 

$$-1 * \frac{\beta_{price} 1/n\sum{P_{i,h,t}P_{j,h,t}} \overline{price_{j}}}{1/n\sum{P_{i,h,t}}}$$

Where $P_{i}$ is the vector of probabilities that each household h chose product i at the training shopping trip t, which is estimated as part of the multinomial logit model, $\overline{price_i}$ is the average price of product i. These elasticites are calculated at the average price, as opposed to calculating all the individual elasticities and averaging them.

Equations summarised from Sismeiro, (2017).

\newpage

##Appendix D - Calculated Elasticities

```{r cache=FALSE}
names(elastdf) <- paste("p",1:20)
rownames(elastdf) <- paste("p",1:20)
kable(elastdf[,1:10])
```

```{r cache=FALSE}
kable(elastdf[,11:20])
```

Where "p i" refers to the ith product in the below table, ordered by share of choice:

\newpage

```{r}
kable(data.frame(number= paste("p",1:20), product = rownames(elastdf_cv)))
```